D:\Python Workspace\Science\stocks>python main.py
STOCK PREDICTION USING RNN LSTM
D:\Python362_64\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
total_data: 3690
yt head :
0    96.80
1    97.15
2    96.25
3    95.10
4    94.95
Name: Close, dtype: float64
          yt     yt_        vt     yt1     yt2     yt3
0      96.80   97.15   4098600   96.25   96.95   94.90
1      97.15   96.25    775400   97.00   97.75   96.80
2      96.25   95.10    669800   97.50   97.50   96.00
3      95.10   94.95    437700   95.85   96.00   95.05
4      94.95   96.25    302400   95.50   95.50   94.05
5      96.25   94.45    524500   95.90   96.90   95.40
6      94.45   94.50   1599700   94.75   94.85   93.40
7      94.50   93.85   1771200   94.75   95.70   94.30
8      93.85   96.55   1380300   94.90   95.00   93.80
9      96.55   95.20  10299800   94.20   96.70   93.95
10     95.20   90.45   4044500   97.05   97.40   94.75
11     90.45   86.95   3630300   95.00   95.40   90.45
12     86.95   91.50   3554500   87.05   91.35   85.95
13     91.50   89.20   2840100   88.55   93.45   88.50
14     89.20   84.75   1875500   91.20   91.50   88.00
15     84.75   81.75   1250000   89.45   89.50   84.75
16     81.75   82.00   2545800   82.40   84.10   80.55
17     82.00   85.25   2748900   82.90   83.65   78.50
18     85.25   83.10   3838600   82.45   85.50   82.00
19     83.10   79.00   8241000   85.50   88.00   82.45
20     79.00   81.00   2470700   82.60   82.70   78.95
21     81.00   83.25   4013900   78.50   82.90   78.25
22     83.25   84.40   3138000   81.25   83.50   81.10
23     84.40   83.75   3369600   83.50   85.00   83.35
24     83.75   84.25   1696000   84.00   84.85   82.55
25     84.25   82.95   2060700   83.80   84.85   83.80
26     82.95   81.75   1246900   83.25   84.45   82.20
27     81.75   80.65   1978600   82.95   83.90   81.60
28     80.65   81.00   1086500   82.00   82.50   79.30
29     81.00   80.00   1470500   80.90   81.20   78.85
...      ...     ...       ...     ...     ...     ...
3659  339.58  328.00    268000  340.00  341.99  337.10
3660  328.00  328.96   1927000  340.10  340.10  324.52
3661  328.96  328.83    473100  330.00  334.49  327.00
3662  328.83  328.27    209100  323.01  330.00  321.00
3663  328.27  326.58    209500  326.90  330.95  325.00
3664  326.58  323.28    175300  328.00  329.90  324.55
3665  323.28  322.27    107700  325.00  328.00  322.00
3666  322.27  319.63    607100  323.05  333.75  319.25
3667  319.63  318.97    475300  320.00  321.75  315.75
3668  318.97  315.14    225000  321.99  321.99  316.56
3669  315.14  319.58    693200  316.51  319.00  315.00
3670  319.58  310.49    242300  310.50  320.30  301.25
3671  310.49  310.26    522800  317.49  317.49  310.00
3672  310.26  319.66    227600  312.84  312.84  307.50
3673  319.66  318.47    405300  308.00  325.00  307.01
3674  318.47  315.42    161200  320.99  322.00  317.00
3675  315.42  314.30    136500  319.00  319.90  315.00
3676  314.30  313.93    107800  314.01  319.00  313.50
3677  313.93  312.92     88400  314.30  315.99  313.50
3678  312.92  311.63    295400  313.75  315.97  311.00
3679  311.63  312.56    388200  312.00  323.10  308.80
3680  312.56  316.72     87600  310.55  314.00  308.11
3681  316.72  313.99    136500  314.00  318.00  313.99
3682  313.99  310.07     61400  314.51  316.84  313.00
3683  310.07  305.57     89000  313.50  314.99  309.50
3684  305.57  297.15    438500  310.07  310.07  296.30
3685  297.15  307.43    403600  303.50  305.00  295.50
3686  307.43  310.40    541800  303.00  310.70  302.00
3687  310.40  307.64    363100  308.95  314.74  300.00
3688  307.64  297.00     90900  309.00  311.95  305.51

[3689 rows x 6 columns]
main.py:102: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(1000, activation="relu", input_shape=(5, 1), recurrent_activation="hard_sigmoid")`
  model.add(LSTM(1000, activation='relu', inner_activation='hard_sigmoid', input_shape=(len(cols), 1)))
main.py:104: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation="linear", units=1)`
  model.add(Dense (output_dim=1, activation='linear'))
D:\Python362_64\lib\site-packages\keras\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 2361 samples, validate on 591 samples
Epoch 1/25
2018-11-13 00:13:29.867448: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2361/2361 [==============================] - 38s 16ms/step - loss: 0.0232 - val_loss: 0.0025
Epoch 2/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0030 - val_loss: 0.0017
Epoch 3/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0023 - val_loss: 0.0014
Epoch 4/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0018 - val_loss: 0.0013
Epoch 5/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0017 - val_loss: 0.0021
Epoch 6/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0020 - val_loss: 0.0013
Epoch 7/25
2361/2361 [==============================] - 36s 15ms/step - loss: 0.0015 - val_loss: 0.0013
Epoch 8/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0014 - val_loss: 0.0012
Epoch 9/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0015 - val_loss: 0.0011
Epoch 10/25
2361/2361 [==============================] - 36s 15ms/step - loss: 0.0013 - val_loss: 0.0015
Epoch 11/25
2361/2361 [==============================] - 36s 15ms/step - loss: 0.0014 - val_loss: 0.0011
Epoch 12/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0013 - val_loss: 0.0012
Epoch 13/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0013 - val_loss: 0.0013
Epoch 14/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0014 - val_loss: 0.0016
Epoch 15/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0013 - val_loss: 0.0017
Epoch 16/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0013 - val_loss: 0.0011
Epoch 17/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0012 - val_loss: 0.0012
Epoch 18/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0012 - val_loss: 0.0013
Epoch 19/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0012 - val_loss: 0.0011
Epoch 20/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0013 - val_loss: 0.0012
Epoch 21/25
2361/2361 [==============================] - 35s 15ms/step - loss: 0.0013 - val_loss: 0.0017
Epoch 22/25
2361/2361 [==============================] - 36s 15ms/step - loss: 0.0014 - val_loss: 0.0011
Epoch 23/25
2361/2361 [==============================] - 41s 17ms/step - loss: 0.0014 - val_loss: 0.0012
Epoch 24/25
2361/2361 [==============================] - 38s 16ms/step - loss: 0.0012 - val_loss: 0.0014
Epoch 25/25
2361/2361 [==============================] - 36s 15ms/step - loss: 0.0012 - val_loss: 0.0017
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_1 (LSTM)                (None, 1000)              4008000
_________________________________________________________________
dropout_1 (Dropout)          (None, 1000)              0
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 1001
=================================================================
Total params: 4,009,001
Trainable params: 4,009,001
Non-trainable params: 0
_________________________________________________________________
None
2952/2952 [==============================] - 66s 22ms/step
736/736 [==============================] - 16s 22ms/step
in train MSE =  0.001
in test MSE =  0.00448026680541539
Saved model to disk
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_1 (LSTM)                (None, 1000)              4008000
_________________________________________________________________
dropout_1 (Dropout)          (None, 1000)              0
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 1001
=================================================================
Total params: 4,009,001
Trainable params: 4,009,001
Non-trainable params: 0
_________________________________________________________________
Inputs: (None, 5, 1)
Outputs: (None, 1)
Actual input: (736, 5, 1)
Actual output: (736, 1)
prediction data:
[300.61322]
actual data
[[3.0339e+02 4.6600e+05 3.0500e+02 3.0575e+02 3.0310e+02]
 [3.0007e+02 7.6030e+05 3.0500e+02 3.0650e+02 2.9899e+02]
 [3.0357e+02 1.3962e+06 2.9998e+02 3.0480e+02 2.9901e+02]
 ...
 [3.0743e+02 5.4180e+05 3.0300e+02 3.1070e+02 3.0200e+02]
 [3.1040e+02 3.6310e+05 3.0895e+02 3.1474e+02 3.0000e+02]
 [3.0764e+02 9.0900e+04 3.0900e+02 3.1195e+02 3.0551e+02]]
D:\Python362_64\lib\site-packages\matplotlib\cbook\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  warnings.warn(message, mplDeprecation, stacklevel=1)
